DOWNLOAD KAFKA, in new versions, zookeeper is coming as bundle inside kafka only, no need to download seperately:
=====================================================================================================================
# Install required packages
sudo apt-get update
sudo apt-get install -y wget net-tools netcat tar openjdk-8-jdk

# Download and extract Kafka
wget https://downloads.apache.org/kafka/$KAFKA_VERSION/kafka_2.12-$KAFKA_VERSION.tgz
tar -xzf kafka_2.12-$KAFKA_VERSION.tgz
ln -s kafka_2.12-$KAFKA_VERSION kafka


SSL ENCRYPTION IN KAFKA:-
=========================

SSL ENCRYPTION IS ONE WAY.....client side encryption and server side decryption
SSL AUTHENTICATION TWO WAY....can server verify back its client?

In keystore, we will have signed server certificate signed by CA
In truststore, we will have ROOT CA public key

the browser/client reads the certificate of server to find the CAs signature, that signed the certificate....and there is a look up for that particular CA's public key in its own trustore... once it finds the public key of that CA, it decrypts the signature with public key to get the checksum string
if the checksum matched, the checksum that the CA had when it was signing and the checksum that is calculated by the browser from the cerificate are the same, if the verification is success, you get a green padlock....Otherwise, you'll get a warning that it is not secure.

STEPS:-
-------

1.CREATE A CA(you will get publec cert and private key)
2.create a server(kafka broker) certificate



cd /home/ubuntu
mkdir ssl
cd ssl

# Setup CA
-----------
create CA => result: file ca-cert(public cert/public key) and the (ca-key)priv.key 
- you can distribute public certificate to anyone who needs to trust your CA..we need to put this public cert in truststore
- you should not share private key to anyone  


cd /home/ubuntu
mkdir ssl
cd ssl 
openssl req -new -newkey rsa:4096 -days 365 -x509 -subj "/CN=Kafka-Security-CA" -keyout ca-key -out ca-cert -nodes

cat ca-cert
cat ca-key
keytool -printcert -v -file ca-cert



### Add-On: public certificates check

echo |
  openssl s_client -connect www.google.com:443 2>/dev/null |
  openssl x509 -noout -text -certopt no_header,no_version,no_serial,no_signame,no_pubkey,no_sigdump,no_aux -subject -nameopt multiline -issuer


-------------------------------------------------------------------------------------------------------------------------------


## create a server(kafka broker) certificate !! put your public EC2-DNS here, after "CN="


export SRVPASS=serversecret       --- env variable to use non interactive commands to create certificates later on
cd ssl

keytool -genkey -keystore kafka.server.keystore.jks -validity 365 -storepass $SRVPASS -keypass $SRVPASS  -dname "CN=ec2-35-165-171-131.us-west-2.compute.amazonaws.com" -storetype pkcs12

- here in the above command, certificate will be created and saved in keystore, so there is one entry in keystore

keytool -list -v -keystore kafka.server.keystore.jks               -- provide keystore password, which is "serversecret"



## create a certification request file, to be signed by the CA

we need to get signed version of certficate for kafka broker, so that all the clients which are there, are able to verify that our kafka broker is valid 

keytool -keystore kafka.server.keystore.jks -certreq -file cert-file -storepass $SRVPASS -keypass $SRVPASS

getting signed version of certficate for kafka broker is a two step process 

1.getting a signing request out from our keystore
  - we now need to send this cert-file signing request certificate to CA, so that CA will able to sign our certificate
2.Getting it signed by CA(own CA for self-signed)


## sign the server certificate => output: file "cert-signed"

openssl x509 -req -CA ca-cert -CAkey ca-key -in cert-file -out cert-signed -days 365 -CAcreateserial -passin pass:$SRVPASS



## check certificates
### our local certificates

keytool -printcert -v -file cert-signed
keytool -list -v -keystore kafka.server.keystore.jks




# Trust the CA by creating a truststore for our kafa broker and importing the ca-cert

keytool -keystore kafka.server.truststore.jks -alias CARoot -import -file ca-cert -storepass $SRVPASS -keypass $SRVPASS -noprompt

- The parameter is just called key store

Certificate(pubic cert/key (ca-cert)) will be added to truststore
- kafka broker at the end will trust all certificates which has been issued by CA



# Import CA and the signed server certificate into the keystore

we need to import new certificates to our keystore 
- initially we created our server certificate which has been put in the kafka.server.keystore.jks
- But now we have a signed version of it, We now need to import this sign certificate into our key store In addition to the CA certificate itself, So both of them need to be need to be stored in the key store.

keytool -keystore kafka.server.keystore.jks -alias CARoot -import -file ca-cert -storepass $SRVPASS -keypass $SRVPASS -noprompt
keytool -keystore kafka.server.keystore.jks -import -file cert-signed -storepass $SRVPASS -keypass $SRVPASS -noprompt



So now we are done with preparing the truststore and the keystore, and we can continue with configuration of our Kafka broker

# Adjust Broker configuration :-
================================ 
Replace the server.properties in AWS, by using the below server.properties file 
Ensure that you set your public-DNS !!

sudo vi /home/ubuntu/kafka/config/server.properties
--------------------------------------------------------------------------------------------------------------------------------
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# see kafka.server.KafkaConfig for additional details and defaults

############################# Server Basics #############################

# The id of the broker. This must be set to a unique integer for each broker.
broker.id=0

############################# Socket Server Settings #############################

# The address the socket server listens on. It will get the value returned from
# java.net.InetAddress.getCanonicalHostName() if not configured.
#   FORMAT:
#     listeners = listener_name://host_name:port
#   EXAMPLE:
#     listeners = PLAINTEXT://your.host.name:9092
listeners=PLAINTEXT://0.0.0.0:9092,SSL://0.0.0.0:9093
advertised.listeners=PLAINTEXT://ec2-18-236-232-157.us-west-2.compute.amazonaws.com:9092,SSL://ec2-18-236-232-157.us-west-2.compute.amazonaws.com:9093
zookeeper.connect=ec2-18-236-232-157.us-west-2.compute.amazonaws.com:2181

ssl.keystore.location=/home/ubuntu/ssl/kafka.server.keystore.jks
ssl.keystore.password=serversecret
ssl.key.password=serversecret
ssl.truststore.location=/home/ubuntu/ssl/kafka.server.truststore.jks
ssl.truststore.password=serversecret


# Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details
#listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL

# The number of threads that the server uses for receiving requests from the network and sending responses to the network
num.network.threads=3

# The number of threads that the server uses for processing requests, which may include disk I/O
num.io.threads=8

# The send buffer (SO_SNDBUF) used by the socket server
socket.send.buffer.bytes=102400

# The receive buffer (SO_RCVBUF) used by the socket server
socket.receive.buffer.bytes=102400

# The maximum size of a request that the socket server will accept (protection against OOM)
socket.request.max.bytes=104857600


auto.create.topics.enable=false

############################# Log Basics #############################

# A comma seperated list of directories under which to store log files
log.dirs=/home/ubuntu/kafka-logs

# The default number of log partitions per topic. More partitions allow greater
# parallelism for consumption, but this will also result in more files across
# the brokers.
num.partitions=1

# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
# This value is recommended to be increased for installations with data dirs located in RAID array.
num.recovery.threads.per.data.dir=1

############################# Internal Topic Settings  #############################
# The replication factor for the group metadata internal topics "_consumer_offsets" and "_transaction_state"
# For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3.
offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1

############################# Log Flush Policy #############################

# Messages are immediately written to the filesystem but by default we only fsync() to sync
# the OS cache lazily. The following configurations control the flush of data to disk.
# There are a few important trade-offs here:
#    1. Durability: Unflushed data may be lost if you are not using replication.
#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.
#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.
# The settings below allow one to configure the flush policy to flush data after a period of time or
# every N messages (or both). This can be done globally and overridden on a per-topic basis.

# The number of messages to accept before forcing a flush of data to disk
#log.flush.interval.messages=10000

# The maximum amount of time a message can sit in a log before we force a flush
#log.flush.interval.ms=1000

############################# Log Retention Policy #############################

# The following configurations control the disposal of log segments. The policy can
# be set to delete segments after a period of time, or after a given size has accumulated.
# A segment will be deleted whenever either of these criteria are met. Deletion always happens
# from the end of the log.

# The minimum age of a log file to be eligible for deletion due to age
log.retention.hours=168

# A size-based retention policy for logs. Segments are pruned from the log unless the remaining
# segments drop below log.retention.bytes. Functions independently of log.retention.hours.
#log.retention.bytes=1073741824

# The maximum size of a log segment file. When this size is reached a new log segment will be created.
log.segment.bytes=1073741824

# The interval at which log segments are checked to see if they can be deleted according
# to the retention policies
log.retention.check.interval.ms=300000

############################# Zookeeper #############################

# Zookeeper connection string (see zookeeper docs for details).
# This is a comma separated host:port pairs, each corresponding to a zk
# server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".
# You can also append an optional chroot string to the urls to specify the
# root directory for all kafka znodes.

# Timeout in ms for connecting to zookeeper
zookeeper.connection.timeout.ms=6000


############################# Group Coordinator Settings #############################

# The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.
# The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms.
# The default value for this is 3 seconds.
# We override this to 0 here as it makes for a better out-of-the-box experience for development and testing.
# However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup.

--------------------------------------------------------------------------------------------------------------------------------




# Restart Kafka

sudo systemctl restart kafka
sudo systemctl status kafka  

# Verify Broker startup

sudo grep "EndPoint" /home/ubuntu/kafka/logs/server.log

sudo journalctl -u kafka | grep SSL

[2022-10-13 18:02:24,777] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://ec2-18-236-232-157.us-west-2.compute.amazonaws.com:9092,SSL://ec2-18-236-232-157.us-west-2.compute.amazonaws.com:9093, czxid (broker epoch): 78 (kafka.zk.KafkaZkClient)

- means broker -id 0 is listening SSL connections on port 9093 and PLAIN connections on 9092


# Adjust SecurityGroup to open port 9093

# Verify SSL config
from your local computer

As a final step for this, we are now going to verify that if we can reach our Kafka broker via the new SSL port(9093)

openssl s_client -connect <<your-public-DNS>>:9093                            
openssl s_client -connect ec2-18-236-232-157.us-west-2.compute.amazonaws.com:9093

s_client   ---    ssh client

openssl s_client -connect ec2-18-236-232-157.us-west-2.compute.amazonaws.com:9093
CONNECTED(00000003)
139750466995520:error:14094410:SSL routines:ssl3_read_bytes:sslv3 alert handshake failure:../ssl/record/rec_layer_s3.c:1543:SSL alert number 40
---
no peer certificate available
---
No client certificate CA names sent
---
SSL handshake has read 7 bytes and written 342 bytes
Verification: OK
---
New, (NONE), Cipher is (NONE)
Secure Renegotiation IS NOT supported
Compression: NONE
Expansion: NONE
No ALPN negotiated
Early data was not sent
Verify return code: 0 (ok)
---

- we are successfully conected with our broker end point on SSL port 9093
- From the security point of view, you should never distribute the ca-key file as well as the kafka.server.keystore.jks file of the Kafka broker.They should be kept very secure and private

- whereas the ca-cert and the cert-signed you can distribute publicly to all the clients and they will import it into their truststore later on to be able to establish an SSL communication successfully.




alias s='sudo systemctl status kafka'
alias r='sudo systemctl restart kafka'
alias l='sudo journalctl -u kafka | tail -n 20'
###############################################################################################################################
###############################################################################################################################


SSL Setup for Clients:-
========================

Take another local server(an aws instance or vagrant local machine or local development box)

# Client configuration for using SSL
we need to create our truststore for our clients, this is needed so that your client can accept and verify the SSL certificate from kafka endpoint

For that to work, we have basically two options.

The one is a more convenient one and that so called chain of trust version, where we are going to import the CA's  public certificate/public key. This means by importing this one to our trust store, we are trusting any service certificate which
has been signed by this CA

The other option would be that you directly import your server certificate, the public one, and by that you would just trust this single server instance and imagine you are going to scale out your Kafka installation to 100 Kafka servers.
You would have to install 100 different SSL certificates into your trust store, and you can imagine
this is a very huge overhead.

So let's download the certificate authoritie's, public certificates.

## grab CA certificate from remote server and add it to local CLIENT truststore


export CLIPASS=clientpass
cd ~
mkdir ssl
cd ssl
scp -i ~/kafka-security.pem ubuntu@<<your-public-DNS>>:/home/ubuntu/ssl/ca-cert .
scp -i ~/kafka-security.pem ubuntu@ec2-35-165-171-131.us-west-2.compute.amazonaws.com:/home/ubuntu/ssl/ca-cert .

keytool -keystore kafka.client.truststore.jks -alias CARoot -import -file ca-cert  -storepass $CLIPASS -keypass $CLIPASS -noprompt

keytool -list -v -keystore kafka.client.truststore.jks


So next we are going to create our properties file as which we have to provide as additional parameter to our clients.
So the console consumer and the console producer at the end.

pwd
/home/ubuntu/ssl
sudo vi client.properties

## create client.properties and configure SSL parameters

security.protocol=SSL
ssl.truststore.location=/home/ubuntu/ssl/kafka.client.truststore.jks
ssl.truststore.password=clientpass

- paste the above content in /home/ubuntu/ssl/client.properties file
- now we can start our clients


## TEST
test using the console-consumer and console-producer 

### Producer

~/kafka/bin/kafka-console-producer.sh --broker-list <<your-public-DNS>>:9093 --topic kafka-security-topic --producer.config ~/ssl/client.properties

~/kafka/bin/kafka-console-producer.sh --broker-list ec2-35-165-171-131.us-west-2.compute.amazonaws.com:9093 --topic kafka-security-topic --producer.config ~/ssl/client.properties


ubuntu@ubuntu-focal:~$ ~/kafka/bin/kafka-console-producer.sh --broker-list ec2-35-165-171-131.us-west-2.compute.amazonaws.com:9093 --topic kafka-security-topic --producer.config ~/ssl/client.properties
>Hi Iam Vijay
>

And you can see that we were able to successfully produce a message to our SSL enabled endpoint. on port 9093


But now let's also test what happens if we don't provide the required cell properties and we are trying to connect to the SSL endpoint of our Kafka broker. So as you can see that the producer still starts up and it tries to add a message, but it doesn't come back to us.

~/kafka/bin/kafka-console-producer.sh --broker-list <<your-public-DNS>>:9093 --topic kafka-security-topic


### Consumer

~/kafka/bin/kafka-console-consumer.sh --bootstrap-server <<your-public-DNS>>:9093 --topic kafka-security-topic --consumer.config ~/ssl/client.properties

~/kafka/bin/kafka-console-consumer.sh --bootstrap-server ec2-35-165-171-131.us-west-2.compute.amazonaws.com:9093 --topic kafka-security-topic --consumer.config ~/ssl/client.properties

ubuntu@ubuntu-focal:~$ ~/kafka/bin/kafka-console-consumer.sh --bootstrap-server ec2-35-165-171-131.us-west-2.compute.amazonaws.com:9093 --topic kafka-security-topic --consumer.config ~/ssl/client.properties
Hi Iam Vijay




PRODUCER AND CONSUMER SIDE BY SIDE:-
-------------------------------------

ubuntu@ubuntu-focal:~$ ~/kafka/bin/kafka-console-producer.sh --broker-list ec2-35-165-171-131.us-west-2.compute.amazonaws.com:9093 --topic kafka-security-topic --producer.config ~/ssl/client.properties
>Hi Iam Vijay
>

ubuntu@ubuntu-focal:~$ ~/kafka/bin/kafka-console-consumer.sh --bootstrap-server ec2-35-165-171-131.us-west-2.compute.amazonaws.com:9093 --topic kafka-security-topic --consumer.config ~/ssl/client.properties
Hi Iam Vijay

################################################################################################################################
################################################################################################################################
