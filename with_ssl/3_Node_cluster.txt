3 NODE ZOOKEEPER AND KAFKA WITH SSSL:-
======================================

take 3 t2.medium instances which are amazon-linux flavoured

SERVER1: ZOOKEEPER1, KAFKA1, CLIENT1
SERVER1: ZOOKEEPER2, KAFKA2, CLIENT2
SERVER1: ZOOKEEPER3, KAFKA3, CLIENT3

allow 22,80,2181,2888-3888,9093 ports


- in 3.0.0 or above kafka version, zookeeper comes by default
Apache Kafka depends on Zookeeper for cluster management. Hence, prior to starting Kafka, Zookeeper has to be started. There is no need to explicitly install Zookeeper, as it comes included with Apache Kafka.
- during coming days in 4.0.0 versions, there is no need of zookeeper

- download kafka from official page


IP ADDRESSES:
=============
SERVER1 35.90.113.2543   ec2-35-90-113-2543.us-west-2.compute.amazonaws.com
SERVER2 52.43.209.2403   ec2-52-43-209-2403.us-west-2.compute.amazonaws.com
SERVER3 54.187.78.1003   ec2-54-187-78-1003.us-west-2.compute.amazonaws.com


IN SERVER1:-
============

STEPS:-
-------

1.CREATE A CA(you will get publec cert and private key)
2.create a server(kafka brokers) certificates

cd /home/ec2-user
mkdir ssl
cd ssl

run the certs.sh scripts by mentioning 3 servers public dns names in it, this script will create certs for SERVER1,SERVER2,SERVER3,client CERTS

SERVER1=ec2-35-90-113-2543.us-west-2.compute.amazonaws.com
SERVER2=ec2-52-43-209-2403.us-west-2.compute.amazonaws.com
SERVER3=ec2-54-187-78-1003.us-west-2.compute.amazonaws.com

SSL certificates are tied to specific domain names, and if the domain names changes, the previously issued certificates will no longer be valid for the new domain names. Therefore, you will have to generate new SSL certificates for the updated domain names and use them to secure your Kafka brokers.

- For example if you created certs for public aws domain names of servers and if you stop server and start again, then the domain name changes, so you need to create certs again.always use the correct SSL certificates corresponding to the current domain names of your AWS instances.

- The SSL certificates themselves do not have built-in awareness of domain name changes. SSL certificates are issued for specific domain names (or IP addresses), and they contain information about the domain they are intended to secure. When a client (e.g., another server, a web browser, or a Kafka client) attempts to connect to your Kafka brokers, it checks the SSL certificate presented by the broker to ensure it matches the domain name the client is trying to reach.

- SSL certificates are issued based on the domain name or IP address for which they are intended to secure communication. The certificate includes the domain name or IP address in its subject field, which is verified by the client (e.g., a web browser, server, or Kafka client) during the SSL handshake process.

- When you create an SSL certificate, you specify the domain name(s) or IP address(es) that the certificate should be valid for. The certificate authority (CA) then verifies the ownership of the domain name(s) before issuing the certificate.

- So, when I mentioned that the SSL certificate contains information about the domain name, it's essentially the name that the certificate is issued for, and the client verifies if the domain name it's trying to connect to matches the domain name in the certificate.

- If the domain name changes (due to instance restart, for example), the previously issued certificate will no longer match the new domain name, and you will need to obtain a new certificate for the updated domain name to ensure secure communication.

- Local Development: If you only need a domain for local development and testing on your computer, you can use the hosts file on your machine to create a temporary domain that points to your local IP address.

vi certs.sh
SERVER1=ec2-35-90-113-2543.us-west-2.compute.amazonaws.com
SERVER2=ec2-52-43-209-2403.us-west-2.compute.amazonaws.com
SERVER3=ec2-54-187-78-1003.us-west-2.compute.amazonaws.com

bash certs.sh

inside ssl folder you will get ca and secrets folder, all the certs and passowrd will be in secrets folder

keytool -v --list --keystore ec2-35-90-113-2543.us-west-2.compute.amazonaws.com.keystore.jks


- so in SERVER1 we have created all certs, now we need to copy truststores and keystores and send to respective servers SERVER2, SERVER3, client if we have any client
 - copy the server private key and paste in secrects folder


cd /home/ec2-user/secrets
vi server.pem                            ---- copy pem file which is used for 3 servers
chmod 400 server.pem

SENDING FOR SERVER1/Local:
--------------------------
cp -r ec2-35-90-113-2543.us-west-2.compute.amazonaws.com.keystore.jks ec2-35-90-113-2543.us-west-2.compute.amazonaws.com.truststore.jks cert_creds /home/ec2-user/ 

SENDING FOR SERVER2:
--------------------
scp -i /home/ec2-user/ssl/secrets/server.pem ec2-52-43-209-2403.us-west-2.compute.amazonaws.com.keystore.jks ec2-52-43-209-2403.us-west-2.compute.amazonaws.com.truststore.jks cert_creds ec2-user@ec2-52-43-209-2403.us-west-2.compute.amazonaws.com:/home/ec2-user/ 

SENDING FOR SERVER3:
--------------------
scp -i /home/ec2-user/ssl/secrets/server.pem ec2-54-187-78-1003.us-west-2.compute.amazonaws.com.keystore.jks ec2-54-187-78-1003.us-west-2.compute.amazonaws.com.truststore.jks cert_creds ec2-user@ec2-54-187-78-1003.us-west-2.compute.amazonaws.com:/home/ec2-user/ 


- now you can check in SERVER2,SERVER3 that there are truststore,keystore,password files 


INSTALL ZOOKEEPER:
==================

Apache Kafka depends on Zookeeper for cluster management. Hence, prior to starting Kafka, Zookeeper has to be started. There is no need to explicitly install Zookeeper, as it comes included with Apache Kafka.
- during coming days in 4.0.0 versions, there is no need of zookeeper


cd /home/ec2-user
sudo yum install java-11-amazon-corretto-headless -y
wget https://downloads.apache.org/kafka/3.5.1/kafka_2.13-3.5.1.tgz
tar -xvf kafka_2.13-3.5.1.tgz
mkdir zookeeper
mkdir kafka-logs
cp -r  kafka_2.13-3.5.1/config/zookeeper.properties kafka_2.13-3.5.1/config/zookeeper.properties-bkp

vi /home/ec2-user/zookeeper/myid
1

vi kafka_2.13-3.5.1/config/zookeeper.properties
tickTime=2000
initLimit=10
syncLimit=5
clientPort=2181
dataDir=/home/ec2-user/zookeeper
maxClientCnxns=60
autopurge.snapRetainCount=3
autopurge.purgeInterval=1
4lw.commands.whitelist=*
server.1=0.0.0.0:2888:3888
server.2=52.43.209.2403:2888:3888
server.3=54.187.78.1003:2888:3888


IN SERVER2:-
=============
cd /home/ec2-user
sudo yum install java-11-amazon-corretto-headless -y
wget https://downloads.apache.org/kafka/3.5.1/kafka_2.13-3.5.1.tgz
tar -xvf kafka_2.13-3.5.1.tgz
mkdir zookeeper
cp -r  kafka_2.13-3.5.1/config/zookeeper.properties kafka_2.13-3.5.1/config/zookeeper.properties-bkp

vi /home/ec2-user/zookeeper/myid
2

vi kafka_2.13-3.5.1/config/zookeeper.properties
tickTime=2000
initLimit=10
syncLimit=5
clientPort=2181
dataDir=/home/ec2-user/zookeeper
maxClientCnxns=60
autopurge.snapRetainCount=3
autopurge.purgeInterval=1
4lw.commands.whitelist=*
server.1=35.90.113.2543:2888:3888
server.2=0.0.0.0:2888:3888
server.3=54.187.78.1003:2888:3888



IN SERVER3:
===========
cd /home/ec2-user
sudo yum install java-11-amazon-corretto-headless -y
wget https://downloads.apache.org/kafka/3.5.1/kafka_2.13-3.5.1.tgz
tar -xvf kafka_2.13-3.5.1.tgz
mkdir zookeeper
cp -r  kafka_2.13-3.5.1/config/zookeeper.properties kafka_2.13-3.5.1/config/zookeeper.properties-bkp

vi /home/ec2-user/zookeeper/myid
3

vi kafka_2.13-3.5.1/config/zookeeper.properties
tickTime=2000
initLimit=10
syncLimit=5
clientPort=2181
dataDir=/home/ec2-user/zookeeper
maxClientCnxns=60
autopurge.snapRetainCount=3
autopurge.purgeInterval=1
4lw.commands.whitelist=*
server.1=35.90.113.2543:2888:3888
server.2=52.43.209.2403:2888:3888
server.3=0.0.0.0:2888:3888



START ZOOKEEPER
===============
start zookeeper in all SERVER1,SERVER2,SERVER3 instances at once

cd kafka_2.13-3.5.1/bin
/home/ec2-user/kafka_2.13-3.5.1/bin/zookeeper-server-start.sh /home/ec2-user/kafka_2.13-3.5.1/config/zookeeper.properties 
the above command starts zookeeper in foreground, if you want to start in background, then use -daemon

/home/ec2-user/kafka_2.13-3.5.1/bin/zookeeper-server-start.sh -daemon /home/ec2-user/kafka_2.13-3.5.1/config/zookeeper.properties 

/home/ec2-user/kafka_2.13-3.5.1/bin/zookeeper-server-stop.sh -daemon /home/ec2-user/kafka_2.13-3.5.1/config/zookeeper.properties


ZOOKEEPER LOGS:
===============
zookeeper logs will be found in 
/home/ec2-user/kafka_2.13-3.5.1/logs
tail -f /home/ec2-user/kafka_2.13-3.5.1/logs/zookeeper.out
tail -f -n 100 /home/ec2-user/kafka_2.13-3.5.1/logs/zookeeper.out
ps -ef | grep zookeeper
netstat -tulpn | grep LISTEN
sudo yum install nc -y
echo stat | nc localhost 2181
- above command 'stat' which is 4 letter(4lw.commands.whitelist=*) is removed from whitelist, but by default they are whitelisted for security reasons.



KAFKA CONFIGURATION:
====================

KAFKA1:
-------
cp -r /home/ec2-user/kafka_2.13-3.5.1/config/server.properties /home/ec2-user/kafka_2.13-3.5.1/config/server.properties-bkp
vi /home/ec2-user/kafka_2.13-3.5.1/config/server.properties
broker.id=1
listeners=SSL://0.0.0.0:9093
#advertised.listeners=SSL://<SERVER1_PUBLIC_DNS>:9093
advertised.listeners=SSL://ec2-35-90-113-2543.us-west-2.compute.amazonaws.com:9093
ssl.keystore.location=/home/ec2-user/ec2-52-43-209-2403.us-west-2.compute.amazonaws.com.keystore.jks
ssl.keystore.password=password
ssl.key.password=password
ssl.truststore.location=/home/ec2-user/ec2-52-43-209-2403.us-west-2.compute.amazonaws.com.truststore.jks
ssl.truststore.password=password
inter.broker.listener.name=SSL
num.network.threads=3
num.io.threads=8
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600
log.dirs=/home/ec2-user/kafka-logs
num.partitions=8
num.recovery.threads.per.data.dir=1
offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1
log.retention.hours=168
log.retention.check.interval.ms=300000
advertised.host.name=localhost
zookeeper.connect=35.90.113.2543:2181,52.43.209.2403:2181,54.187.78.1003:2181
zookeeper.connection.timeout.ms=18000
group.initial.rebalance.delay.ms=0



KAFKA2:
-------
cp -r /home/ec2-user/kafka_2.13-3.5.1/config/server.properties /home/ec2-user/kafka_2.13-3.5.1/config/server.properties-bkp
vi /home/ec2-user/kafka_2.13-3.5.1/config/server.properties
broker.id=2
listeners=SSL://0.0.0.0:9093
#advertised.listeners=SSL://<SERVER2_PUBLIC_DNS>:9093
advertised.listeners=SSL://ec2-52-43-209-2403.us-west-2.compute.amazonaws.com:9093
ssl.keystore.location=/home/ec2-user/ec2-52-43-209-2403.us-west-2.compute.amazonaws.com.keystore.jks
ssl.keystore.password=password
ssl.key.password=password
ssl.truststore.location=/home/ec2-user/ec2-52-43-209-2403.us-west-2.compute.amazonaws.com.truststore.jks
ssl.truststore.password=password
inter.broker.listener.name=SSL
num.network.threads=3
num.io.threads=8
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600
log.dirs=/home/ec2-user/kafka-logs
num.partitions=8
num.recovery.threads.per.data.dir=1
offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1
log.retention.hours=168
log.retention.check.interval.ms=300000
advertised.host.name=localhost
zookeeper.connect=35.90.113.2543:2181,52.43.209.2403:2181,54.187.78.1003:2181
zookeeper.connection.timeout.ms=18000
group.initial.rebalance.delay.ms=0



KAFKA3:
-------
cp -r /home/ec2-user/kafka_2.13-3.5.1/config/server.properties /home/ec2-user/kafka_2.13-3.5.1/config/server.properties-bkp
vi /home/ec2-user/kafka_2.13-3.5.1/config/server.properties
broker.id=3
listeners=SSL://0.0.0.0:9093
#advertised.listeners=SSL://<SERVER3_PUBLIC_DNS>:9093
advertised.listeners=SSL://ec2-54-187-78-1003.us-west-2.compute.amazonaws.com:9093
ssl.keystore.location=/home/ec2-user/ec2-54-187-78-1003.us-west-2.compute.amazonaws.com.keystore.jks
ssl.keystore.password=password
ssl.key.password=password
ssl.truststore.location=/home/ec2-user/ec2-54-187-78-1003.us-west-2.compute.amazonaws.com.truststore.jks
ssl.truststore.password=password
inter.broker.listener.name=SSL
num.network.threads=3
num.io.threads=8
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600
log.dirs=/home/ec2-user/kafka-logs
num.partitions=8
num.recovery.threads.per.data.dir=1
offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1
log.retention.hours=168
log.retention.check.interval.ms=300000
advertised.host.name=localhost
zookeeper.connect=35.90.113.2543:2181,52.43.209.2403:2181,54.187.78.1003:2181
zookeeper.connection.timeout.ms=18000
group.initial.rebalance.delay.ms=0



START KAFKA:-
=============
start kafka in all SERVER1,SERVER2,SERVER3 instances at once

/home/ec2-user/kafka_2.13-3.5.1/bin/kafka-server-start.sh /home/ec2-user/kafka_2.13-3.5.1/config/server.properties

/home/ec2-user/kafka_2.13-3.5.1/bin/kafka-server-start.sh -daemon /home/ec2-user/kafka_2.13-3.5.1/config/server.properties

KAFKA LOGS:-
============
tail -f -n 100 /home/ec2-user/kafka_2.13-3.5.1/logs/server.log

[2023-07-30 18:25:45,935] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)
[2023-07-30 18:25:46,149] INFO [zk-broker-0-to-controller-forwarding-channel-manager]: Recorded new controller, from now on will use node localhost:9092 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2023-07-30 18:25:46,150] INFO [zk-broker-0-to-controller-alter-partition-channel-manager]: Recorded new controller, from now on will use node localhost:9092 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread)


[ec2-user@kafka1 logs]$ echo dump | nc localhost 2181 | grep brokers
        /brokers/ids/1
        /brokers/ids/2
        /brokers/ids/3

/home/ec2-user/kafka_2.13-3.5.1/bin/kafka-server-stop.sh /home/ec2-user/kafka_2.13-3.5.1/config/server.properties



CONFIGURING ZOOKEEPER AND KAFKA AS SERVICE:-
============================================
execute in all 3 SERVER1,SERVER2,SERVER3 servers

ZOOKEEPER:
----------
sudo vi /etc/systemd/system/zookeeper.service
[Unit]
Description=Apache ZooKeeper
Documentation=http://zookeeper.apache.org
Requires=network.target remote-fs.target
After=network.target remote-fs.target
[Service]
Type=simple
User=ec2-user
Group=ec2-user
ExecStart=/home/ec2-user/kafka_2.13-3.5.1/bin/zookeeper-server-start.sh /home/ec2-user/kafka_2.13-3.5.1/config/zookeeper.properties
ExecStop=/home/ec2-user/kafka_2.13-3.5.1/bin/zookeeper-server-stop.sh
Restart=on-failure
RestartSec=5s
[Install]
WantedBy=multi-user.target

sudo systemctl daemon-reload
sudo systemctl enable zookeeper
sudo systemctl start zookeeper
sudo systemctl status zookeeper
sudo systemctl stop zookeeper

KAFKA:
------
sudo vi /etc/systemd/system/kafka.service
[Unit]
Description=Apache Kafka
Documentation=http://kafka.apache.org
Requires=zookeeper.service
After=zookeeper.service
[Service]
Type=simple
User=ec2-user
Group=ec2-user
ExecStart=/home/ec2-user/kafka_2.13-3.5.1/bin/kafka-server-start.sh /home/ec2-user/kafka_2.13-3.5.1/config/server.properties
# Uncomment the following line if you want Kafka to run as a background (daemon) process
# ExecStart=/home/ec2-user/kafka_2.13-3.5.1/bin/kafka-server-start.sh -daemon /home/ec2-user/kafka_2.13-3.5.1/config/server.properties
ExecStop=/home/ec2-user/kafka_2.13-3.5.1/bin/kafka-server-stop.sh
Restart=on-failure
RestartSec=5s
[Install]
WantedBy=multi-user.target

sudo systemctl daemon-reload
sudo systemctl enable kafka
sudo systemctl start kafka
sudo systemctl status kafka
sudo systemctl stop kafka



[ec2-user@kafka1 ~]$ echo dump | nc localhost 2181
SessionTracker dump:
Global Sessions(3):
0x10002d7158b0003       18000ms
0x20002d711b50004       18000ms
0x30002d71fc60005       18000ms
ephemeral nodes dump:
Sessions with Ephemerals (3):
0x30002d71fc60005:
        /brokers/ids/2
0x20002d711b50004:
        /brokers/ids/3
0x10002d7158b0003:
        /controller
        /brokers/ids/1
Connections dump:
Connections Sets (2)/(2):
0 expire at Tue Aug 01 16:32:27 UTC 2023:
2 expire at Tue Aug 01 16:32:37 UTC 2023:
        ip: /127.0.0.1:33308 sessionId: 0x0
        ip: /35.90.113.254:52988 sessionId: 0x10002d7158b0003
#############################################################################################################################################################
#############################################################################################################################################################

SSL Setup for Clients:-
========================

Take another local server(an aws instance or vagrant local machine or local development box)
- we have already created certificates for client in SERVER1 by using script certs.sh,and password for all is present in cert_creds file, now we need to copy those cetificates to all clients which are needed
- for now,
we will take SERVER1 as client1
we will take SERVER2 as client2
we will take SERVER3 as client3

SENDING FOR SERVER1/Local:
--------------------------
cd /home/ec2-user/ssl/secrets
cp -r client.keystore.jks client.truststore.jks /home/ec2-user/

SENDING FOR SERVER2:
--------------------
scp -i /home/ec2-user/ssl/secrets/server.pem client.keystore.jks client.truststore.jks ec2-user@ec2-52-43-209-2403.us-west-2.compute.amazonaws.com:/home/ec2-user/ 

SENDING FOR SERVER3:
--------------------
scp -i /home/ec2-user/ssl/secrets/server.pem client.keystore.jks client.truststore.jks ec2-user@ec2-54-187-78-1003.us-west-2.compute.amazonaws.com:/home/ec2-user/ 


## create client.properties and configure SSL parameters in all clients i.e SERVER1,SERVER2,SERVER3
cd /home/ec2-user
vi client.properties

bootstrap.servers=ec2-35-90-113-2543.us-west-2.compute.amazonaws.com:9093,ec2-52-43-209-2403.us-west-2.compute.amazonaws.com:9093,ec2-54-187-78-1003.us-west-2.compute.amazonaws.com:9093
security.protocol=SSL
ssl.truststore.location=/home/ec2-user/client.truststore.jks
ssl.truststore.password=password
ssl.keystore.location=/home/ec2-user/client.keystore.jks
ssl.keystore.password=password




##################################################################################################################

## TESTING PRODUCER AND CONSUMER:-
===================================
test using the console-consumer and console-producer 

finding kafka version:
----------------------
[ec2-user@kafka1 bin]$ ./kafka-server-start.sh --version
[2023-08-01 17:27:39,580] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
3.5.1

create topic:
-------------
/home/ec2-user/kafka_2.13-3.5.1/bin/kafka-topics.sh --bootstrap-server ec2-35-90-113-254.us-west-2.compute.amazonaws.com:9093,ec2-52-43-209-240.us-west-2.compute.amazonaws.com:9093,ec2-54-187-78-100.us-west-2.compute.amazonaws.com:9093 \
--command-config /home/ec2-user/client.properties \
--create \
--topic myTopic \
--replication-factor 3 \
--partitions 5

list topics:
------------
/home/ec2-user/kafka_2.13-3.5.1/bin/kafka-topics.sh --bootstrap-server ec2-35-90-113-254.us-west-2.compute.amazonaws.com:9093,ec2-52-43-209-240.us-west-2.compute.amazonaws.com:9093,ec2-54-187-78-100.us-west-2.compute.amazonaws.com:9093 --list --command-config /home/ec2-user/client.properties

[ec2-user@kafka1 bin]$ /home/ec2-user/kafka_2.13-3.5.1/bin/kafka-topics.sh --bootstrap-server ec2-35-90-113-254.us-west-2.compute.amazonaws.com:9093,ec2-52-43-209-240.us-west-2.compute.amazonaws.com:9093,ec2-54-187-78-100.us-west-2.compute.amazonaws.com:9093 --list --command-config /home/ec2-user/client.properties

myTopic

describe topic:
---------------
/home/ec2-user/kafka_2.13-3.5.1/bin/kafka-topics.sh --bootstrap-server ec2-35-90-113-254.us-west-2.compute.amazonaws.com:9093,ec2-52-43-209-240.us-west-2.compute.amazonaws.com:9093,ec2-54-187-78-100.us-west-2.compute.amazonaws.com:9093 \
--command-config /home/ec2-user/client.properties \
--describe \
--topic myTopic

Topic: myTopic  TopicId: 998AOHpgRrGoj0g6iBRjcg PartitionCount: 3       ReplicationFactor: 3    Configs: 
        Topic: myTopic  Partition: 0    Leader: 3       Replicas: 3,1,2 Isr: 3,1,2
        Topic: myTopic  Partition: 1    Leader: 1       Replicas: 1,2,3 Isr: 1,2,3
        Topic: myTopic  Partition: 2    Leader: 2       Replicas: 2,3,1 Isr: 2,3,1


### Producer
ON SERVER1/CLIENT1:
-------------------
/home/ec2-user/kafka_2.13-3.5.1/bin/kafka-console-producer.sh --bootstrap-server ec2-35-90-113-254.us-west-2.compute.amazonaws.com:9093,ec2-52-43-209-240.us-west-2.compute.amazonaws.com:9093,ec2-54-187-78-100.us-west-2.compute.amazonaws.com:9093 --topic myTopic --producer.config /home/ec2-user/client.properties

[ec2-user@kafka1 bin]$ /home/ec2-user/kafka_2.13-3.5.1/bin/kafka-console-producer.sh --bootstrap-server ec2-35-90-113-254.us-west-2.compute.amazonaws.com:9093,ec2-52-43-209-240.us-west-2.compute.amazonaws.com:9093,ec2-54-187-78-100.us-west-2.compute.amazonaws.com:9093 --topic myTopic --producer.config /home/ec2-user/client.properties
>Hi, Vijay producer is here, are you ready to consume ??
>

And you can see that we were able to successfully produce a message to our SSL enabled endpoint. on port 9093

But now let's also test what happens if we don't provide the required cell properties and we are trying to connect to the SSL endpoint of our Kafka broker. So as you can see that the producer still starts up and it tries to add a message, but it doesn't come back to us.

~/kafka/bin/kafka-console-producer.sh --broker-list <<your-public-DNS>>:9093 --topic kafka-security-topic


### Consumer
ON SERVER2/CLIENT2:
-------------------
/home/ec2-user/kafka_2.13-3.5.1/bin/kafka-console-consumer.sh --bootstrap-server ec2-35-90-113-254.us-west-2.compute.amazonaws.com:9093,ec2-52-43-209-240.us-west-2.compute.amazonaws.com:9093,ec2-54-187-78-100.us-west-2.compute.amazonaws.com:9093 --topic myTopic --consumer.config /home/ec2-user/client.properties --from-beginning

[ec2-user@kafka2 ~]$ /home/ec2-user/kafka_2.13-3.5.1/bin/kafka-console-consumer.sh --bootstrap-server ec2-35-90-113-254.us-west-2.compute.amazonaws.com:9093,ec2-52-43-209-240.us-west-2.compute.amazonaws.com:9093,ec2-54-187-78-100.us-west-2.compute.amazonaws.com:9093 --topic myTopic --consumer.config /home/ec2-user/client.properties --from-beginning
Hi Vijay producer is here, are you ready to consume ??


### Describe topic
ON SERVER3/CLIENT3:
-------------------
/home/ec2-user/kafka_2.13-3.5.1/bin/kafka-topics.sh --bootstrap-server ec2-35-90-113-254.us-west-2.compute.amazonaws.com:9093,ec2-52-43-209-240.us-west-2.compute.amazonaws.com:9093,ec2-54-187-78-100.us-west-2.compute.amazonaws.com:9093 --describe --topic myTopic --command-config /home/ec2-user/client.properties

Topic: myTopic  TopicId: F7X8QcVWTn6OMJW1VWS17A PartitionCount: 1       ReplicationFactor: 1    Configs: 
        Topic: myTopic  Partition: 0    Leader: 1       Replicas: 1     Isr: 1


### Consumer groups
====================
consumer groups:-
-----------------
- every consumer is associated with some consumer group by default, if no group name is provided
- while we create a consumer without mentioning any consumer group name, then kafka will create a default consumer group

list consumer groups:-
----------------------
before executing below command, we need to make sure that there is active consumer running, otherwise, command will show empty, which means there are no active consumers running, run the consumer by below command and after that execute the consumer group list command

/home/ec2-user/kafka_2.13-3.5.1/bin/kafka-console-consumer.sh --bootstrap-server ec2-35-90-113-254.us-west-2.compute.amazonaws.com:9093,ec2-52-43-209-240.us-west-2.compute.amazonaws.com:9093,ec2-54-187-78-100.us-west-2.compute.amazonaws.com:9093 --topic myTopic --consumer.config /home/ec2-user/client.properties --from-beginning

/home/ec2-user/kafka_2.13-3.5.1/bin/kafka-consumer-groups.sh --bootstrap-server ec2-35-90-113-254.us-west-2.compute.amazonaws.com:9093,ec2-52-43-209-240.us-west-2.compute.amazonaws.com:9093,ec2-54-187-78-100.us-west-2.compute.amazonaws.com:9093 --list --command-config /home/ec2-user/client.properties

console-consumer-67119

- kafka will create consumer group by default with some numberID like:- console-consumer-68120


describe consumer group:-
-------------------------
before executing below command, we need to make sure that there is active consumer running, otherwise, command will show empty, which means there are no active consumers running

/home/ec2-user/kafka_2.13-3.5.1/bin/kafka-consumer-groups.sh --bootstrap-server ec2-35-90-113-254.us-west-2.compute.amazonaws.com:9093,ec2-52-43-209-240.us-west-2.compute.amazonaws.com:9093,ec2-54-187-78-100.us-west-2.compute.amazonaws.com:9093 --describe --group console-consumer-67119 --command-config /home/ec2-user/client.properties

GROUP                  TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                           HOST            CLIENT-ID
console-consumer-67119 myTopic         0          -               2               -               console-consumer-ca39d906-f656-4241-80dd-2836f43b2ddf /172.31.6.221   console-consumer

- here LOG-END-OFFSET=2, because till now we have 2 messages in that topic
- The entire consumer groups information will be stored by kafka cluster in the form of seperate offsets topic
- inside the __consumer_offsets topic, we will have information about all consumers, like each consumer from where messages are consumed from which topic etc...

while executing above commad, you may get errors related to java heap space OOM

The java.lang.OutOfMemoryError: Java heap space error occurs when the Java Virtual Machine (JVM) does not have enough heap space to perform its operations. The heap space is where the JVM stores objects during program execution. When it runs out of memory in this space, it throws an OutOfMemoryError.

export KAFKA_HEAP_OPTS="-Xmx2g -Xms1g"

The above command sets the maximum heap size (-Xmx) to 2 GB and the initial heap size (-Xms) to 1 GB. You can adjust these values according to the available memory on your system.
Additionally, if you continue to encounter the TimeoutException, make sure that the Kafka brokers are running and accessible at the specified bootstrap server addresses and ports. 


multiple producers and consumers on same topic:-
================================================
producer1/client1/SERVER1:
--------------------------
/home/ec2-user/kafka_2.13-3.5.1/bin/kafka-console-producer.sh --bootstrap-server ec2-35-90-113-254.us-west-2.compute.amazonaws.com:9093,ec2-52-43-209-240.us-west-2.compute.amazonaws.com:9093,ec2-54-187-78-100.us-west-2.compute.amazonaws.com:9093 --topic myTopic --producer.config /home/ec2-user/client.properties
>producer1

producer2/client2/SERVER2:
--------------------------
/home/ec2-user/kafka_2.13-3.5.1/bin/kafka-console-producer.sh --bootstrap-server ec2-35-90-113-254.us-west-2.compute.amazonaws.com:9093,ec2-52-43-209-240.us-west-2.compute.amazonaws.com:9093,ec2-54-187-78-100.us-west-2.compute.amazonaws.com:9093 --topic myTopic --producer.config /home/ec2-user/client.properties
>producer2

consumer3/client3/SERVER3:
--------------------------
/home/ec2-user/kafka_2.13-3.5.1/bin/kafka-console-consumer.sh --bootstrap-server ec2-35-90-113-254.us-west-2.compute.amazonaws.com:9093,ec2-52-43-209-240.us-west-2.compute.amazonaws.com:9093,ec2-54-187-78-100.us-west-2.compute.amazonaws.com:9093 --topic myTopic --consumer.config /home/ec2-user/client.properties --from-beginning

producer1
producer2
- here, we have ran two consumer without defining consumer groups, so kafka will randomly generate consumer groups and will add those in that groups


consumer3/client3/SERVER3(creating own consumer group and SUBSCRIBING TO TOPIC):-
---------------------------------------------------------------------------------
/home/ec2-user/kafka_2.13-3.5.1/bin/kafka-console-consumer.sh --bootstrap-server ec2-35-90-113-254.us-west-2.compute.amazonaws.com:9093,ec2-52-43-209-240.us-west-2.compute.amazonaws.com:9093,ec2-54-187-78-100.us-west-2.compute.amazonaws.com:9093 --topic myTopic --group myConsumerGroup --from-beginning --consumer.config /home/ec2-user/client.properties
 
producer1
producer2


remove a topic:
---------------
/home/ec2-user/kafka_2.13-3.5.1/bin/kafka-topics.sh --bootstrap-server ec2-35-90-113-254.us-west-2.compute.amazonaws.com:9093,ec2-52-43-209-240.us-west-2.compute.amazonaws.com:9093,ec2-54-187-78-100.us-west-2.compute.amazonaws.com:9093 \
--command-config /home/ec2-user/client.properties \
--delete \
--topic myTopic

- YOU CANNOT DELETE A TOPIC WHICH IS ALREADY CONSUMING OR PRODUCING BY OTHERS



CONTROLLER:-
============
KAFKA CONTROLLER NODE:
----------------------
In kafka cluster, one of the broker server will servers as controller, which is responsible for managing the states of partitions and replicas and for performing administrative tasks like reassigning partitions

[ec2-user@kafka1 bin]$ echo dump | nc localhost 2181
SessionTracker dump:
Global Sessions(3):
0x10002d7158b0003       18000ms
0x20002d711b50004       18000ms
0x30002d71fc60005       18000ms
ephemeral nodes dump:
Sessions with Ephemerals (3):
0x30002d71fc60005:
        /brokers/ids/2
0x20002d711b50004:
        /brokers/ids/3
0x10002d7158b0003:
        /controller
        /brokers/ids/1

- In every kafka cluster, there will be one contoller Node, here it is  /controller/brokers/ids/1 broker1

- LETS CREATE A TOPIC WITH 3 PARTITION AND 2 REPLICATION:

/home/ec2-user/kafka_2.13-3.5.1/bin/kafka-topics.sh --bootstrap-server ec2-35-90-113-254.us-west-2.compute.amazonaws.com:9093,ec2-52-43-209-240.us-west-2.compute.amazonaws.com:9093,ec2-54-187-78-100.us-west-2.compute.amazonaws.com:9093 \
--command-config /home/ec2-user/client.properties \
--create \
--topic myTopic \
--replication-factor 2 \
--partitions 3

/home/ec2-user/kafka_2.13-3.5.1/bin/kafka-topics.sh --bootstrap-server ec2-35-90-113-254.us-west-2.compute.amazonaws.com:9093,ec2-52-43-209-240.us-west-2.compute.amazonaws.com:9093,ec2-54-187-78-100.us-west-2.compute.amazonaws.com:9093 \
--command-config /home/ec2-user/client.properties \
--describe \
--topic myTopic

Topic: myTopic  TopicId: G3jKfSyuRUiIHF1MDQH1QA PartitionCount: 3       ReplicationFactor: 2    Configs: 
        Topic: myTopic  Partition: 0    Leader: 3       Replicas: 3,1   Isr: 3,1
        Topic: myTopic  Partition: 1    Leader: 1       Replicas: 1,2   Isr: 1,2
        Topic: myTopic  Partition: 2    Leader: 2       Replicas: 2,3   Isr: 2,3

Topic: myTopic  Partition: 0    Leader: 3       Replicas: 3,1   Isr: 3,1
The above line states that, for Partition0, broker3 is the leader. And Replicas means, the Partition0 is present in both broker3 and broker1.
all the read and write operations will be done from broker3 for Partition0 because for Partition0, broker3 is the leader. The main Partition0 replica which is broker3 is the leader replica and Partition0 in broker1 is the follower replica....so if broker3 is down, then the Partition0 replica in broker1 will become main Partition for reading and writing data.
so if any producer and consumer wants to write and read data to Partition0 of myTopic topic, then it should contact broker3.
Isr(in-sync-replicas) means, Isr is the subset of Replicas, so here    Replicas: 3,1   Isr: 3,1
means, broker3 and broker1 are in-sync in data transmission i.e same Partition0 data is present in broker3 and broker1
For example producer is producing data to Partition0 of broker3 which is the leader, so the Partition0 of broker1 will send fetch request to Partition0 of broker3 to copy the data from Partition0 of broker3 to Partition0 of broker1


what happens internally when we create a topic:-
------------------------------------------------
Partition state:
----------------
1.NonExistentPartition: this state indicates that the partition was either never created or was created and then deleted
2.NewPartition: After creation, the partition is in the NewPartition state, in this state the partition should have replicas assigned to it, but no leader/isr yet assigned
3.OnlinePartition: once a leader is elected for a partition, it is in the OnlinePartition state. we can read/write in this state only
4.OfflinePartition: if, after successfull leader election, the leader for partition dies, then the Partition moves to the OfflinePartition state

Replica state:
--------------
1.NewReplica: when replicas are created during topic creation or partition reassignment, in this state, a replica can only get become follower state change request
2.OnlineReplica: once a replica is started and part of the assigned replicas for its partition, it is in this state. in this state , it can get either become leader become follower state change request
3.OfflineReplica: if a replica dies, it moves to this state, This happens when the broker hosting the replica is down
4.NonExistentReplica: if a replica is deleted, it is moved to this state

- these all states will be managed by the CONTROLLER NODE, here it is broker1, 
- there will be only one controller node in entire kafka cluster
- so all partitions states in the cluster will be mananged by the controller
- controller will also manage state transition i.e transferring from one state to other 
